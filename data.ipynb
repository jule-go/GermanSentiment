{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Jule Godbersen](mailto:godbersj@tcd.ie)\n",
    "\n",
    "Content of file: Loading, preprocessing and analyzing the data used within this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from data_analysis import analyze_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the [MMS dataset](https://brand24-ai.github.io/mms_benchmark/)\n",
    "\n",
    "Note: As the dataset is private, log in to your huggingface account and then accept the terms of use for the [Huggingface website of the dataset](https://huggingface.co/datasets/Brand24/mms). Also make sure to have run ``huggingface-cli login`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Brand24/mms\",cache_dir=\"/mount/studenten-temp1/users/godberja/HuggingfaceCache\") # TODO adapt path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection\n",
    "Within this project I make use of the MMS dataset which is a multimodal set and contains data for multiple languages. Within this project I will only make use of the English and German data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset a bit first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "print(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the documentation I found that the labels are represented via indices within the dataset\n",
    "label2ind = {\"negative\":0,\"neutral\":1,\"positive\":2}\n",
    "ind2label = {0:\"negative\",1:\"neutral\",2:\"positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 0,\n",
       " 'text': '\" آللهمَ إني أسألك من الخير كله ..عاجله وآجله ، ما علمت منه وما لم أعلم،،وأعوذ بك من الشر كله ..عاجله وآجله، ما علمت منه وما لم أعلم \"',\n",
       " 'label': 2,\n",
       " 'original_dataset': 'ar_arsentdl',\n",
       " 'domain': 'social_media',\n",
       " 'language': 'ar',\n",
       " 'Family': 'Afro-Asiatic',\n",
       " 'Genus': 'Semitic',\n",
       " 'Definite articles': 'definite affix',\n",
       " 'Indefinite articles': 'no article',\n",
       " 'Number of cases': '3',\n",
       " 'Order of subject, object, verb': 'SVO',\n",
       " 'Negative morphemes': 'negative particle',\n",
       " 'Polar questions': 'interrogative intonation only',\n",
       " 'Position of negative word wrt SOV': 'SNegVO',\n",
       " 'Prefixing vs suffixing': 'weakly suffixing',\n",
       " 'Coding of nominal plurality': 'mixed morphological plural',\n",
       " 'Grammatical genders': 'masculine, feminine',\n",
       " 'cleanlab_self_confidence': 0.21071475744247437}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at an example data sample\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for distribution of different languages supported / labels\n",
    "languages = []\n",
    "labels = []\n",
    "for data_sample in dataset[\"train\"]:\n",
    "    languages += [data_sample[\"language\"]]\n",
    "    labels += [data_sample[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data samples across languages:  Counter({'en': 2330486, 'ar': 932075, 'es': 418712, 'zh': 331702, 'de': 315887, 'pl': 236688, 'fr': 210631, 'ja': 209780, 'cs': 196287, 'pt': 157834, 'sl': 113543, 'ru': 110930, 'hr': 77594, 'sr': 76368, 'th': 72319, 'bg': 62150, 'hu': 56682, 'sk': 56623, 'sq': 44284, 'sv': 41346, 'bs': 36183, 'ur': 19660, 'hi': 16999, 'fa': 13525, 'it': 12065, 'he': 8619, 'lv': 5790, 'el': 500})\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of data samples across languages: \",Counter(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data samples across sentiment classes:  Counter({2: 3494710, 1: 1341392, 0: 1329160})\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of data samples across sentiment classes: \",Counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset for languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only get the German data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset for german only\n",
    "german = dataset.filter(lambda row: row[\"language\"] == \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 315887 data samples available in German.\n"
     ]
    }
   ],
   "source": [
    "print(\"There is\",len(german[\"train\"]),\"data samples available in German.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources of datasets:  Counter({'de_multilan_amazon': 209073, 'de_twitter_sentiment': 90534, 'de_sb10k': 9948, 'de_omp': 3598, 'de_dai_labor': 1781, 'de_ifeel': 953})\n"
     ]
    }
   ],
   "source": [
    "# just out of interest: check dataset sources for german\n",
    "german_sources = []\n",
    "for datasample in german[\"train\"]:\n",
    "    german_sources += [datasample[\"original_dataset\"]]\n",
    "print(\"Sources of datasets: \",Counter(german_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is quite a lot of data and I want to simulate a low-resource scenario: I will limit the amount of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only get the English data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = dataset.filter(lambda row: row[\"language\"] == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 2330486 data samples available in English.\n"
     ]
    }
   ],
   "source": [
    "print(\"There is\",len(english[\"train\"]),\"data samples available in English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources of datasets:  Counter({'en_amazon': 1883238, 'en_multilan_amazon': 209393, 'en_twitter_sentiment': 85784, 'en_semeval_2017': 65071, 'en_tweet_airlines': 14427, 'en_silicone_meld_s': 12138, 'en_sentistrength': 11759, 'en_vader_movie_reviews': 10605, 'en_dai_labor': 7073, 'en_per_sent': 5333, 'en_vader_nyt': 5190, 'en_silicone_sem': 4643, 'en_vader_twitter': 4200, 'en_vader_amazon': 3708, 'en_financial_phrasebank_sentences_75agree': 3448, 'en_tweets_sanders': 3424, 'en_poem_sentiment': 1052})\n"
     ]
    }
   ],
   "source": [
    "# just out of interest: check dataset sources for english\n",
    "english_sources = []\n",
    "for datasample in english[\"train\"]:\n",
    "    english_sources += [datasample[\"original_dataset\"]]\n",
    "print(\"Sources of datasets: \",Counter(english_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there's way too much data available: Check if there's datasets were there's samples for German as well as for English. Then later take a subset of these each.\n",
    "\n",
    "(Because remember: I want to simulate a resource-scenario that means I simulate the scenario of having a smaller amount of data in German. Of course normally one would use as much data as being made available. But this would not help me answering my research questions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multilan_amazon\n",
      "twitter_sentiment\n",
      "dai_labor\n"
     ]
    }
   ],
   "source": [
    "# compare which dataset sources are common for both languages\n",
    "en_s = {'en_amazon': 1883238, 'en_multilan_amazon': 209393, 'en_twitter_sentiment': 85784, 'en_semeval_2017': 65071, 'en_tweet_airlines': 14427, 'en_silicone_meld_s': 12138, 'en_sentistrength': 11759, 'en_vader_movie_reviews': 10605, 'en_dai_labor': 7073, 'en_per_sent': 5333, 'en_vader_nyt': 5190, 'en_silicone_sem': 4643, 'en_vader_twitter': 4200, 'en_vader_amazon': 3708, 'en_financial_phrasebank_sentences_75agree': 3448, 'en_tweets_sanders': 3424, 'en_poem_sentiment': 1052}\n",
    "de_s = {'de_multilan_amazon': 209073, 'de_twitter_sentiment': 90534, 'de_sb10k': 9948, 'de_omp': 3598, 'de_dai_labor': 1781, 'de_ifeel': 953}\n",
    "en_s_c = [k[3:] for k in en_s]\n",
    "for k in de_s:\n",
    "    if k[3:] in en_s_c:\n",
    "        print(k[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find more information about the datasets on the papers mentioned on the following website: https://brand24-ai.github.io/mms_benchmark/citations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have different \"genres\" available I want to use multilan_amazon and twitter_sentiment further on. I disregard dai_labor as it similarly to twitter_sentiment contains texts from social media. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon review dataset with  209393  English samples\n",
      "Amazon review dataset with  209073  German samples\n",
      "Amazon review dataset with  85784  English samples\n",
      "Amazon review dataset with  90534  German samples\n"
     ]
    }
   ],
   "source": [
    "# extract datasets and print their sizes\n",
    "en_reviews = dataset.filter(lambda row: row[\"original_dataset\"] == \"en_multilan_amazon\")\n",
    "print(\"Amazon review dataset with \",len(en_reviews[\"train\"]),\" English samples\")\n",
    "de_reviews = dataset.filter(lambda row: row[\"original_dataset\"] == \"de_multilan_amazon\")\n",
    "print(\"Amazon review dataset with \",len(de_reviews[\"train\"]),\" German samples\")\n",
    "\n",
    "en_twitter = dataset.filter(lambda row: row[\"original_dataset\"] == \"en_twitter_sentiment\")\n",
    "print(\"Amazon review dataset with \",len(en_twitter[\"train\"]),\" English samples\")\n",
    "de_twitter = dataset.filter(lambda row: row[\"original_dataset\"] == \"de_twitter_sentiment\")\n",
    "print(\"Amazon review dataset with \",len(de_twitter[\"train\"]),\" German samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of datasamples and creation of datasplits\n",
    "\n",
    "I now have access to German and English data. They are all within one training split and are probably imbalanced with respect to class labels. Thus I will now balance the data and split it into 3 splits for training, development and testing. The ratio I choose is 70:20:10.\n",
    "\n",
    "Note that I ignore the dataset source when balancing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract ids of the instances in given data dataset\n",
    "def get_ids(data):\n",
    "    return [instance[\"_id\"] for instance in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to balance the dataset for having the same amount of instances for each label\n",
    "def balance_data(data):\n",
    "\n",
    "    # split data according to labels\n",
    "    neg_instances = data.filter(lambda row: row[\"label\"]==0)[\"train\"]\n",
    "    all_neg_ids = get_ids(neg_instances)\n",
    "    neut_instances = data.filter(lambda row: row[\"label\"]==1)[\"train\"]\n",
    "    all_neut_ids = get_ids(neut_instances)\n",
    "    pos_instances = data.filter(lambda row: row[\"label\"]==2)[\"train\"]\n",
    "    all_pos_ids = get_ids(pos_instances)\n",
    "\n",
    "    # get the minimum amount of instances we can extract for each of the labels \n",
    "    class_minimum = min(len(all_neg_ids),len(all_neut_ids),len(all_pos_ids))\n",
    "    print(\"We have \",class_minimum,\" instances for all three labels\")\n",
    "\n",
    "    # shuffle list with ids to make sure we randomly choose instances\n",
    "    random.shuffle(all_neg_ids)\n",
    "    random.shuffle(all_neut_ids)\n",
    "    random.shuffle(all_pos_ids)\n",
    "\n",
    "    # extract class_minimum amount of (shuffled!) instances for each of the labels\n",
    "    neg_ids = all_neg_ids[:class_minimum]\n",
    "    neut_ids = all_neut_ids[:class_minimum]\n",
    "    pos_ids = all_pos_ids[:class_minimum]\n",
    "\n",
    "    return neg_ids,neut_ids,pos_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  41935  instances for all three labels\n",
      "There is  41935  instances for each label in the German review data\n",
      "We have  17447  instances for all three labels\n",
      "There is  17447  instances for each label in the German twitter data\n"
     ]
    }
   ],
   "source": [
    "# balance the German data\n",
    "de_review_neg, de_review_neut, de_review_pos = balance_data(de_reviews)\n",
    "print(\"There is \",len(de_review_neg),\" instances for each label in the German review data\")\n",
    "de_twitter_neg, de_twitter_neut, de_twitter_pos = balance_data(de_twitter)\n",
    "print(\"There is \",len(de_twitter_neg),\" instances for each label in the German twitter data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  41940  instances for all three labels\n",
      "There is  41940  instances for each label in the English review data\n",
      "We have  22829  instances for all three labels\n",
      "There is  22829  instances for each label in the English twitter data\n"
     ]
    }
   ],
   "source": [
    "# balance the English data\n",
    "en_review_neg, en_review_neut, en_review_pos = balance_data(en_reviews)\n",
    "print(\"There is \",len(en_review_neg),\" instances for each label in the English review data\")\n",
    "en_twitter_neg, en_twitter_neut, en_twitter_pos = balance_data(en_twitter)\n",
    "print(\"There is \",len(en_twitter_neg),\" instances for each label in the English twitter data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't find online how many datasamples are actually needed for training a model. To represent the low-resource scenario I will make use of 600 German instances (which is ideally 100 per \"genre\" and this for each class label). The high resource will be represented with 6000 English instances (which is ideally 1000 per \"genre\" and this for each class label). These numbers refer to the size of the training data. The test split should always be the same and should contain 120 instances (balanced for the three classes). Note that I want to have a ratio of ca. 70:30 for train-dev.\n",
    "\n",
    "If there is time left after my experiments, I want to experiment further with the dataset size. Thus I also create a small, a medium and a large training and development split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the datasets to counterfact any patterns in dataset\n",
    "random.shuffle(de_twitter_neg)\n",
    "random.shuffle(de_twitter_neut)\n",
    "random.shuffle(de_twitter_pos)\n",
    "random.shuffle(en_twitter_neg)\n",
    "random.shuffle(en_twitter_neut)\n",
    "random.shuffle(en_twitter_pos)\n",
    "random.shuffle(de_review_neg)\n",
    "random.shuffle(de_review_neut)\n",
    "random.shuffle(de_review_pos)\n",
    "random.shuffle(en_review_neg)\n",
    "random.shuffle(en_review_neut)\n",
    "random.shuffle(en_review_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(ids,size):\n",
    "    split = ids[:size]\n",
    "    left_ids = ids[size:] # remove ids from list\n",
    "    return split,left_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set the values I decided to choose\n",
    "test_size = 20 # leads to: 20 instances * 3 labels * 2 dataset sources = 120 test instances\n",
    "train_size_small = 100 # leads to: 100 instances * 3 labels * 2 dataset sources = 600 train instances\n",
    "train_size_medium = 500 # leads to: 500 instances * 3 labels * 2 dataset sources = 3000 train instances\n",
    "train_size_large = 1000 # leads to: 1000 instances * 3 labels * 2 dataset sources = 6000 train instances\n",
    "train_dev_ratio = [70,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_splits(ids,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio):\n",
    "    # calculate size of data split\n",
    "    dev_size_large = int(train_size_large / train_dev_ratio[0] * train_dev_ratio[1])\n",
    "    dev_size_medium = int(train_size_medium / train_dev_ratio[0] * train_dev_ratio[1])\n",
    "    dev_size_small = int(train_size_small / train_dev_ratio[0] * train_dev_ratio[1])\n",
    "\n",
    "    test, ids = create_split(ids,test_size)\n",
    "\n",
    "    train_large, ids = create_split(ids,train_size_large)\n",
    "    train_medium, _ = create_split(train_large,train_size_medium)\n",
    "    train_small, _ = create_split(train_large,train_size_small)\n",
    "\n",
    "    dev_large, _ = create_split(ids,dev_size_large)\n",
    "    dev_medium, _ = create_split(dev_large,dev_size_medium)\n",
    "    dev_small, _ = create_split(dev_large,dev_size_small)\n",
    "\n",
    "    return test, train_large, train_medium, train_small, dev_large, dev_medium, dev_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splits for German twitter\n",
    "\n",
    "# look at negative samples\n",
    "de_twitter_neg_test, de_twitter_neg_train_large, de_twitter_neg_train_medium, de_twitter_neg_train_small, de_twitter_neg_dev_large, de_twitter_neg_dev_medium, de_twitter_neg_dev_small = extract_splits(de_twitter_neg,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at neutral samples\n",
    "de_twitter_neut_test, de_twitter_neut_train_large, de_twitter_neut_train_medium, de_twitter_neut_train_small, de_twitter_neut_dev_large, de_twitter_neut_dev_medium, de_twitter_neut_dev_small = extract_splits(de_twitter_neut,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at positive samples\n",
    "de_twitter_pos_test, de_twitter_pos_train_large, de_twitter_pos_train_medium, de_twitter_pos_train_small, de_twitter_pos_dev_large, de_twitter_pos_dev_medium, de_twitter_pos_dev_small = extract_splits(de_twitter_pos,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splits for English twitter\n",
    "\n",
    "# look at negative samples\n",
    "en_twitter_neg_test, en_twitter_neg_train_large, en_twitter_neg_train_medium, en_twitter_neg_train_small, en_twitter_neg_dev_large, en_twitter_neg_dev_medium, en_twitter_neg_dev_small = extract_splits(en_twitter_neg,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at neutral samples\n",
    "en_twitter_neut_test, en_twitter_neut_train_large, en_twitter_neut_train_medium, en_twitter_neut_train_small, en_twitter_neut_dev_large, en_twitter_neut_dev_medium, en_twitter_neut_dev_small = extract_splits(en_twitter_neut,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at positive samples\n",
    "en_twitter_pos_test, en_twitter_pos_train_large, en_twitter_pos_train_medium, en_twitter_pos_train_small, en_twitter_pos_dev_large, en_twitter_pos_dev_medium, en_twitter_pos_dev_small = extract_splits(en_twitter_pos,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splits for German review\n",
    "\n",
    "# look at negative samples\n",
    "de_review_neg_test, de_review_neg_train_large, de_review_neg_train_medium, de_review_neg_train_small, de_review_neg_dev_large, de_review_neg_dev_medium, de_review_neg_dev_small = extract_splits(de_review_neg,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at neutral samples\n",
    "de_review_neut_test, de_review_neut_train_large, de_review_neut_train_medium, de_review_neut_train_small, de_review_neut_dev_large, de_review_neut_dev_medium, de_review_neut_dev_small = extract_splits(de_review_neut,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at positive samples\n",
    "de_review_pos_test, de_review_pos_train_large, de_review_pos_train_medium, de_review_pos_train_small, de_review_pos_dev_large, de_review_pos_dev_medium, de_review_pos_dev_small = extract_splits(de_review_pos,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splits for English review\n",
    "\n",
    "# look at negative samples\n",
    "en_review_neg_test, en_review_neg_train_large, en_review_neg_train_medium, en_review_neg_train_small, en_review_neg_dev_large, en_review_neg_dev_medium, en_review_neg_dev_small = extract_splits(en_review_neg,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at neutral samples\n",
    "en_review_neut_test, en_review_neut_train_large, en_review_neut_train_medium, en_review_neut_train_small, en_review_neut_dev_large, en_review_neut_dev_medium, en_review_neut_dev_small = extract_splits(en_review_neut,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)\n",
    "\n",
    "# look at positive samples\n",
    "en_review_pos_test, en_review_pos_train_large, en_review_pos_train_medium, en_review_pos_train_small, en_review_pos_dev_large, en_review_pos_dev_medium, en_review_pos_dev_small = extract_splits(en_review_pos,test_size,train_size_small,train_size_medium,train_size_large,train_dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge splits, shuffle lists and save ids in a dict\n",
    "all_ids = dict()\n",
    "\n",
    "# test files\n",
    "de_test = de_twitter_neg_test + de_twitter_neut_test + de_twitter_pos_test + de_review_neg_test + de_review_neut_test + de_review_pos_test\n",
    "random.shuffle(de_test)\n",
    "all_ids[\"de_test\"] = de_test\n",
    "\n",
    "en_test = en_twitter_neg_test + en_twitter_neut_test + en_twitter_pos_test + en_review_neg_test + en_review_neut_test + en_review_pos_test\n",
    "random.shuffle(en_test)\n",
    "all_ids[\"en_test\"] = en_test\n",
    "\n",
    "# train files large\n",
    "de_train_large = de_twitter_neg_train_large + de_twitter_neut_train_large + de_twitter_pos_train_large + de_review_neg_train_large + de_review_neut_train_large + de_review_pos_train_large\n",
    "random.shuffle(de_train_large)\n",
    "all_ids[\"de_train_large\"] = de_train_large\n",
    "\n",
    "en_train_large = en_twitter_neg_train_large + en_twitter_neut_train_large + en_twitter_pos_train_large + en_review_neg_train_large + en_review_neut_train_large + en_review_pos_train_large\n",
    "random.shuffle(en_train_large)\n",
    "all_ids[\"en_train_large\"] = en_train_large\n",
    "\n",
    "# train files medium\n",
    "de_train_medium = de_twitter_neg_train_medium + de_twitter_neut_train_medium + de_twitter_pos_train_medium + de_review_neg_train_medium + de_review_neut_train_medium + de_review_pos_train_medium\n",
    "random.shuffle(de_train_medium)\n",
    "all_ids[\"de_train_medium\"] = de_train_medium\n",
    "\n",
    "en_train_medium = en_twitter_neg_train_medium + en_twitter_neut_train_medium + en_twitter_pos_train_medium + en_review_neg_train_medium + en_review_neut_train_medium + en_review_pos_train_medium\n",
    "random.shuffle(en_train_medium)\n",
    "all_ids[\"en_train_medium\"] = en_train_medium\n",
    "\n",
    "# train files small\n",
    "de_train_small = de_twitter_neg_train_small + de_twitter_neut_train_small + de_twitter_pos_train_small + de_review_neg_train_small + de_review_neut_train_small + de_review_pos_train_small\n",
    "random.shuffle(de_train_small)\n",
    "all_ids[\"de_train_small\"] = de_train_small\n",
    "\n",
    "en_train_small = en_twitter_neg_train_small + en_twitter_neut_train_small + en_twitter_pos_train_small + en_review_neg_train_small + en_review_neut_train_small + en_review_pos_train_small\n",
    "random.shuffle(en_train_small)\n",
    "all_ids[\"en_train_small\"] = en_train_small\n",
    "\n",
    "# dev files large\n",
    "de_dev_large = de_twitter_neg_dev_large + de_twitter_neut_dev_large + de_twitter_pos_dev_large + de_review_neg_dev_large + de_review_neut_dev_large + de_review_pos_dev_large\n",
    "random.shuffle(de_dev_large)\n",
    "all_ids[\"de_dev_large\"] = de_dev_large\n",
    "\n",
    "en_dev_large = en_twitter_neg_dev_large + en_twitter_neut_dev_large + en_twitter_pos_dev_large + en_review_neg_dev_large + en_review_neut_dev_large + en_review_pos_dev_large\n",
    "random.shuffle(en_dev_large)\n",
    "all_ids[\"en_dev_large\"] = en_dev_large\n",
    "\n",
    "# dev files medium\n",
    "de_dev_medium = de_twitter_neg_dev_medium + de_twitter_neut_dev_medium + de_twitter_pos_dev_medium + de_review_neg_dev_medium + de_review_neut_dev_medium + de_review_pos_dev_medium\n",
    "random.shuffle(de_dev_medium)\n",
    "all_ids[\"de_dev_medium\"] = de_dev_medium\n",
    "\n",
    "en_dev_medium = en_twitter_neg_dev_medium + en_twitter_neut_dev_medium + en_twitter_pos_dev_medium + en_review_neg_dev_medium + en_review_neut_dev_medium + en_review_pos_dev_medium\n",
    "random.shuffle(en_dev_medium)\n",
    "all_ids[\"en_dev_medium\"] = en_dev_medium\n",
    "\n",
    "# dev files small\n",
    "de_dev_small = de_twitter_neg_dev_small + de_twitter_neut_dev_small + de_twitter_pos_dev_small + de_review_neg_dev_small + de_review_neut_dev_small + de_review_pos_dev_small\n",
    "random.shuffle(de_dev_small)\n",
    "all_ids[\"de_dev_small\"] = de_dev_small\n",
    "\n",
    "en_dev_small = en_twitter_neg_dev_small + en_twitter_neut_dev_small + en_twitter_pos_dev_small + en_review_neg_dev_small + en_review_neut_dev_small + en_review_pos_dev_small\n",
    "random.shuffle(en_dev_small)\n",
    "all_ids[\"en_dev_small\"] = en_dev_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_test :  120  instances\n",
      "en_test :  120  instances\n",
      "de_train_large :  6000  instances\n",
      "en_train_large :  6000  instances\n",
      "de_train_medium :  3000  instances\n",
      "en_train_medium :  3000  instances\n",
      "de_train_small :  600  instances\n",
      "en_train_small :  600  instances\n",
      "de_dev_large :  2568  instances\n",
      "en_dev_large :  2568  instances\n",
      "de_dev_medium :  1284  instances\n",
      "en_dev_medium :  1284  instances\n",
      "de_dev_small :  252  instances\n",
      "en_dev_small :  252  instances\n"
     ]
    }
   ],
   "source": [
    "# once again look at the length of the splits\n",
    "for k,v in all_ids.items():\n",
    "    print(k, \": \",len(v),\" instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indices to file\n",
    "with open('data/ids.pkl', 'wb') as file:\n",
    "    pickle.dump(all_ids, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I still need to translate the datasets. I'm using the google translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to get the indices by loading from file, use the following:\n",
    "with open('data/ids.pkl', 'rb') as file:\n",
    "    all_ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2e_translator = GoogleTranslator(source=\"de\",target=\"en\")\n",
    "e2g_translator = GoogleTranslator(source=\"en\",target=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate a text represented as string with help of the given translator\n",
    "def translate_text(translator, text):\n",
    "    return translator.translate(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sehr guter Film...spannend , sehenswert\n",
      "Very good film...exciting, worth seeing\n",
      "\n",
      "Book was good, easy to read, plot was low keyed. Could read but put down and come back to read later . Was not a thriller ,you could not put down. Just a good book.\n",
      "Das Buch war gut, leicht zu lesen, die Handlung war zurückhaltend. Konnte es lesen, aber zur Seite legen und später wiederkommen, um es zu lesen. War kein Thriller, den man nicht aus der Hand legen konnte. Einfach ein gutes Buch.\n"
     ]
    }
   ],
   "source": [
    "# test if translation works using some example instances\n",
    "german_example = dataset[\"train\"][all_ids[\"de_test\"][0]][\"text\"]\n",
    "print(german_example)\n",
    "print(translate_text(g2e_translator,german_example))\n",
    "print(\"\")\n",
    "english_example = dataset[\"train\"][all_ids[\"en_test\"][0]][\"text\"]\n",
    "print(english_example)\n",
    "print(translate_text(e2g_translator,english_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you', 'I like you', 'You are awesome']\n"
     ]
    }
   ],
   "source": [
    "# also try if the translate_batch command works\n",
    "t_pl = [\"Ich hab dich lieb\",\"Ich mag dich\",\"Du bist toll\"]\n",
    "translated = GoogleTranslator('de', 'en').translate_batch(t_pl)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_ids(data,ids,src,tgt):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data: Dataset from huggingface\n",
    "        ids (list): list of ids\n",
    "        src (str): represents source language\n",
    "        tgt (str): represents target language\n",
    "\n",
    "    Returns:\n",
    "        dict: key = text in source language, value = translated text into target language\n",
    "    \"\"\"\n",
    "    texts = [data[\"train\"][data_id][\"text\"][:4999] for data_id in ids]\n",
    "    translated_texts = GoogleTranslator(src,tgt).translate_batch(texts)\n",
    "    translations = dict()\n",
    "    for i in range(len(ids)):\n",
    "        translations[ids[i]] = translated_texts[i] # key = id, value = translation\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_train_translation done\n",
      "de_dev_translation done\n",
      "de_test_translation done\n",
      "en_train_translation done\n",
      "en_dev_translation done\n",
      "en_test_translation done\n"
     ]
    }
   ],
   "source": [
    "# do batched translations and save them to file\n",
    "\n",
    "# translate german to english\n",
    "de_train_translation = translate_ids(dataset,all_ids[\"de_train_large\"],\"de\",\"en\")\n",
    "print(\"de_train_translation done\")\n",
    "de_dev_translation = translate_ids(dataset,all_ids[\"de_dev_large\"],\"de\",\"en\")\n",
    "print(\"de_dev_translation done\")\n",
    "de_test_translation = translate_ids(dataset,all_ids[\"de_test\"],\"de\",\"en\")\n",
    "print(\"de_test_translation done\")\n",
    "\n",
    "# translate english to german\n",
    "en_train_translation = translate_ids(dataset,all_ids[\"en_train_large\"],\"en\",\"de\")\n",
    "print(\"en_train_translation done\")\n",
    "en_dev_translation = translate_ids(dataset,all_ids[\"en_dev_large\"],\"en\",\"de\")\n",
    "print(\"en_dev_translation done\")\n",
    "en_test_translation = translate_ids(dataset,all_ids[\"en_test\"],\"en\",\"de\")\n",
    "print(\"en_test_translation done\")\n",
    "\n",
    "translations = dict(**de_train_translation,**de_dev_translation,**de_test_translation,**en_train_translation,**en_dev_translation,**en_test_translation)\n",
    "\n",
    "with open(\"data/translations.pkl\", 'wb') as file:\n",
    "    pickle.dump(translations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6978/210147 [14:57<7:15:30,  7.78it/s] \n"
     ]
    },
    {
     "ename": "TooManyRequests",
     "evalue": "Server Error: You made too many requests to the server.According to google, you are allowed to make 5 requests per secondand up to 200k requests per day. You can wait and try again later oryou can try the translate_batch function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m translations:\n\u001b[1;32m     15\u001b[0m     text \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][ind][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m4999\u001b[39m] \u001b[38;5;66;03m# max amount of chars is 5000\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     translations[ind] \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# save intermediate progress of translations to file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(translations) \u001b[38;5;241m%\u001b[39m checkpoint_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(translator, text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_text\u001b[39m(translator, text):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mount/studenten-temp1/users/godberja/.cslp_proj_env/lib64/python3.8/site-packages/deep_translator/google.py:71\u001b[0m, in \u001b[0;36mGoogleTranslator.translate\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_params, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n",
      "\u001b[0;31mTooManyRequests\u001b[0m: Server Error: You made too many requests to the server.According to google, you are allowed to make 5 requests per secondand up to 200k requests per day. You can wait and try again later oryou can try the translate_batch function"
     ]
    }
   ],
   "source": [
    "# # translate german text content of according indices and save translations to files\n",
    "# language = \"de_test\"\n",
    "# translations = dict()\n",
    "# translator = g2e_translator\n",
    "# target = \"english\"\n",
    "# checkpoint_frequency = 1000\n",
    "# for split in ids[language].keys():\n",
    "#     checkpoint_filename = \"data/\" + language + \"2\" + target + \"_\" + split+ \"_checkpoint.pkl\"\n",
    "#     filename = \"data/\" + language + \"2\" + target + \"_\" + split+ \".pkl\"\n",
    "#     if os.path.exists(checkpoint_filename):\n",
    "#         with open(checkpoint_filename, 'rb') as file:\n",
    "#             translations = pickle.load(file)\n",
    "#     for ind in tqdm(ids[language][split]):\n",
    "#         if ind not in translations:\n",
    "#             text = dataset[\"train\"][ind][\"text\"][:4999] # max amount of chars is 5000\n",
    "#             translations[ind] = translate_text(translator,text=text)\n",
    "#             # save intermediate progress of translations to file\n",
    "#         if len(translations) % checkpoint_frequency == 0: \n",
    "#             with open(checkpoint_filename, 'wb') as file:\n",
    "#                 pickle.dump(translations, file)\n",
    "#             time.sleep(60) # wait a minute until translating again\n",
    "\n",
    "#     if set(translations.keys()) == set(ids[language][split]):\n",
    "#         print(\"Finished translating \",split, \" split from \",language,\" into \",target)\n",
    "#     else:\n",
    "#         print(\"Unfortunately there are some IDs missing\")\n",
    "#         break\n",
    "    \n",
    "#     # save final file version\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(translations, file)\n",
    "    \n",
    "#     os.remove(checkpoint_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the file ``data_analysis.py`` to analyze the dataset for e.g. distribution of labels across datasplits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cslp_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
