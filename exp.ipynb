{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/godberja/.cslp_proj_env/lib64/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import data_loading as data_loading\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mount/studenten-temp1/users/godberja/GermanSentiment/data/ids.pkl', 'rb') as file:\n",
    "    ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = ids[\"de_train_small\"]\n",
    "dev_ids = ids[\"de_dev_small\"]\n",
    "test_ids = ids[\"de_test\"] \n",
    "train_large_ids = ids[\"de_train_large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Brand24/mms\",cache_dir=\"/mount/studenten-temp1/users/godberja/HuggingfaceCache\")\n",
    "train_data = data_loading.load_own_dataset(dataset,train_ids,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_large = data_loading.load_own_dataset(dataset,train_ids,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(datasplit,split:str):\n",
    "    neg = 0\n",
    "    neut = 0\n",
    "    pos = 0\n",
    "    for sample in datasplit:\n",
    "        if sample[\"label\"] == \"0\":\n",
    "            neg += 1\n",
    "        elif sample[\"label\"] == \"1\":\n",
    "            neut += 1\n",
    "        else:\n",
    "            pos += 1\n",
    "    all = neg + neut + pos\n",
    "    return split+\": \"+str(round(neg/(all)*100))+ \"%\\t[\"+str(neg)+\"] negative instances\\t\\t\" +str(round(neut/(all)*100))+ \"%\\t[\"+str(neut)+\"] neutral instances\\t\\t\" +str(round(pos/(all)*100))+ \"%\\t[\"+str(pos)+\"] positive instances\\t\\t\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large: 0%\\t[0] negative instances\\t\\t0%\\t[0] neutral instances\\t\\t100%\\t[600] positive instances\\t\\t\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_distribution(train_data_large,\"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "dict_keys(['text', 'label', 'id'])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data_large:\n",
    "    print(type(sample[\"label\"]))\n",
    "    print(sample.keys())\n",
    "    # print(sample[\"original_dataset\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "peek of closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mount/studenten-temp1/users/godberja/GermanSentiment/evaluation/predictions_german_test_0.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: peek of closed file"
     ]
    }
   ],
   "source": [
    "with open(\"/mount/studenten-temp1/users/godberja/GermanSentiment/evaluation/predictions_german_test_0.pkl\",\"rb\") as infile:\n",
    "    predictions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/mount/studenten-temp1/users/godberja/GermanSentiment/evaluation/predictions_german_test_0.pkl', 'rb') as file:\n",
    "    predictions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Brand24/mms\",cache_dir=\"/mount/studenten-temp1/users/godberja/HuggingfaceCache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_quantitatively(predictions, dataset, print=False, path_to_save_evaluation_to=None):\n",
    "\n",
    "    # investigate general performance of model\n",
    "    preds = [datapoint[\"pred\"] for datapoint in predictions]\n",
    "    golds = [datapoint[\"gold\"] for datapoint in predictions]\n",
    "    accuracy = \"Accuracy: \"+str(metrics.accuracy_score(golds, preds))\n",
    "    f_score = \"F1-score: \"+str(metrics.f1_score(golds, preds, average=\"weighted\"))\n",
    "\n",
    "    # investigate confidence of model\n",
    "    label_probs = [max(datapoint[\"prob\"]) for datapoint in predictions]\n",
    "    avg_prob = round(sum(label_probs) / len(label_probs),3)\n",
    "    min_prob = min(label_probs)\n",
    "    max_prob = max(label_probs)\n",
    "    median_prob = round(statistics.median(label_probs),3)\n",
    "    std_prob = round(statistics.stdev(label_probs),3)\n",
    "    confidence = \"Confidence of model: \"+str(avg_prob)+\"[avg]\\t\"+str(std_prob)+\"[std dev]\\t\"+str(min_prob)+\"[min]\\t\"+str(max_prob)+\"[max]\\t\"+str(median_prob)+\"[median]\"\n",
    "\n",
    "    # generate a confusion matrix \n",
    "    conf_matrix = np.array2string(metrics.confusion_matrix(golds,preds,labels=[\"0\",\"1\",\"2\"]))\n",
    "    confusion = \"Confusion matrix: \\n\"+str(conf_matrix)\n",
    "    ConfusionMatrixDisplay.from_predictions(golds,preds)\n",
    "    plt.title(str(\"Confusion Matrix\"))\n",
    "    if path_to_save_evaluation_to:\n",
    "        plt.savefig(str(path_to_save_evaluation_to+\"_confusionMatrix.jpg\")) \n",
    "    plt.close()\n",
    "\n",
    "    # look at difference across \"genre\"\n",
    "    ids = [datapoint[\"id\"] for datapoint in predictions]\n",
    "    sources = [dataset[\"train\"][data_id][\"original_dataset\"] for data_id in ids]\n",
    "    twitter_preds = []\n",
    "    twitter_golds = []\n",
    "    review_preds = []\n",
    "    review_golds = []\n",
    "    for i, src in enumerate(sources):\n",
    "        if \"amazon\" in src:\n",
    "            review_preds += [predictions[i][\"pred\"]]\n",
    "            review_golds += [predictions[i][\"gold\"]]\n",
    "        else: # \"twitter\" in src:\n",
    "            twitter_preds += [predictions[i][\"pred\"]]\n",
    "            twitter_golds += [predictions[i][\"gold\"]]\n",
    "    twitter_accuracy = \"Accuracy on twitter data only: \"+str(metrics.accuracy_score(twitter_golds, twitter_preds))\n",
    "    review_accuracy = \"Accuracy on review data only: \"+str(metrics.accuracy_score(review_golds, review_preds))  \n",
    "    conf_matrix_twitter = np.array2string(metrics.confusion_matrix(twitter_golds,twitter_preds,labels=[\"0\",\"1\",\"2\"]))\n",
    "    confusion_twitter = \"Confusion matrix of twitter data: \\n\"+str(conf_matrix_twitter)\n",
    "    plt.clf()\n",
    "    ConfusionMatrixDisplay.from_predictions(twitter_golds,twitter_preds)\n",
    "    plt.title(str(\"Confusion Matrix Twitter data\"))\n",
    "    if path_to_save_evaluation_to:\n",
    "        plt.savefig(str(path_to_save_evaluation_to+\"_confusionMatrix_twitter.jpg\")) \n",
    "    plt.close()\n",
    "    conf_matrix_review = np.array2string(metrics.confusion_matrix(review_golds,review_preds,labels=[\"0\",\"1\",\"2\"]))\n",
    "    confusion_review = \"Confusion matrix of review data: \\n\"+str(conf_matrix_review)\n",
    "    plt.clf()\n",
    "    ConfusionMatrixDisplay.from_predictions(review_golds,review_preds)\n",
    "    plt.title(str(\"Confusion Matrix Review data\"))\n",
    "    if path_to_save_evaluation_to:\n",
    "        plt.savefig(str(path_to_save_evaluation_to+\"_confusionMatrix_review.jpg\")) \n",
    "    plt.close()\n",
    "\n",
    "    # analyze the model performance with the built-in classification_report function\n",
    "    report = \"Built-in analysis: \\n\"+str(metrics.classification_report(y_true=golds,y_pred=preds))+\"\\n\"\n",
    "\n",
    "    if print:\n",
    "        print(accuracy)\n",
    "        print(f_score)\n",
    "        print(confidence)\n",
    "        print(confusion)\n",
    "        print(report)\n",
    "        print(\"\\nLet's look at differences between 'genres':\")\n",
    "        print(twitter_accuracy)\n",
    "        print(review_accuracy)\n",
    "        print(confusion_twitter)\n",
    "        print(confusion_review)\n",
    "    \n",
    "    if path_to_save_evaluation_to:\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "if x:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.16666666666666666)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_quantitatively(predictions,dataset,\"/mount/studenten-temp1/users/godberja/GermanSentiment/evaluation/model0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1239901, 'text': 'Anhänger war nach 6 Tagen kaputt', 'gold': '1', 'pred': '0', 'prob': [0.338, 0.334, 0.327]}\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [datapoint[\"id\"] for datapoint in predictions]\n",
    "sources = [dataset[\"train\"][data_id][\"original_dataset\"] for data_id in ids]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'de_twitter_sentiment', 'de_multilan_amazon'}\n"
     ]
    }
   ],
   "source": [
    "print(set(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 1239901,\n",
       " 'text': 'Anhänger war nach 6 Tagen kaputt',\n",
       " 'label': 1,\n",
       " 'original_dataset': 'de_multilan_amazon',\n",
       " 'domain': 'reviews',\n",
       " 'language': 'de',\n",
       " 'Family': 'Indo-European',\n",
       " 'Genus': 'Germanic',\n",
       " 'Definite articles': 'definite word distinct from demonstrative',\n",
       " 'Indefinite articles': 'indefinite word same as one',\n",
       " 'Number of cases': '4',\n",
       " 'Order of subject, object, verb': 'no dominant order',\n",
       " 'Negative morphemes': 'negative particle',\n",
       " 'Polar questions': 'interrogative word order',\n",
       " 'Position of negative word wrt SOV': 'more than one position',\n",
       " 'Prefixing vs suffixing': 'strongly suffixing',\n",
       " 'Coding of nominal plurality': 'plural suffix',\n",
       " 'Grammatical genders': 'masculine, feminine, neuter',\n",
       " 'cleanlab_self_confidence': 0.0547509491443634}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][1239901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cslp_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
