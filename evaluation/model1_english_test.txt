Accuracy: 0.575
F1-score: 0.5828814451845271
Confidence of model: 0.587[avg]	0.129[std dev]	0.347[min]	0.944[max]	0.579[median]
Confusion matrix: 
[[27 13  0]
 [11 22  7]
 [ 1 19 20]]
Built-in analysis: 
              precision    recall  f1-score   support

           0       0.69      0.68      0.68        40
           1       0.41      0.55      0.47        40
           2       0.74      0.50      0.60        40

    accuracy                           0.57       120
   macro avg       0.61      0.58      0.58       120
weighted avg       0.61      0.57      0.58       120


Let's look at differences between 'genres':
Accuracy on twitter data only: 0.45
Accuracy on review data only: 0.7
Confusion matrix of twitter data: 
[[10 10  0]
 [ 4 13  3]
 [ 1 15  4]]
Confusion matrix of review data: 
[[17  3  0]
 [ 7  9  4]
 [ 0  4 16]]
