Accuracy: 0.49166666666666664
F1-score: 0.45016095016095015
Confidence of model: 0.54[avg]	0.094[std dev]	0.348[min]	0.779[max]	0.522[median]
Confusion matrix: 
[[ 9 28  3]
 [ 1 38  1]
 [ 2 26 12]]
Built-in analysis: 
              precision    recall  f1-score   support

           0       0.75      0.23      0.35        40
           1       0.41      0.95      0.58        40
           2       0.75      0.30      0.43        40

    accuracy                           0.49       120
   macro avg       0.64      0.49      0.45       120
weighted avg       0.64      0.49      0.45       120


Let's look at differences between 'genres':
Accuracy on twitter data only: 0.43333333333333335
Accuracy on review data only: 0.55
Confusion matrix of twitter data: 
[[ 2 16  2]
 [ 1 19  0]
 [ 1 14  5]]
Confusion matrix of review data: 
[[ 7 12  1]
 [ 0 19  1]
 [ 1 12  7]]
