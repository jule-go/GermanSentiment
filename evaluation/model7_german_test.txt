Accuracy: 0.49166666666666664
F1-score: 0.48393517052053636
Confidence of model: 0.453[avg]	0.078[std dev]	0.343[min]	0.728[max]	0.439[median]
Confusion matrix: 
[[21  8 11]
 [15 13 12]
 [ 6  9 25]]
Built-in analysis: 
              precision    recall  f1-score   support

           0       0.50      0.53      0.51        40
           1       0.43      0.33      0.37        40
           2       0.52      0.62      0.57        40

    accuracy                           0.49       120
   macro avg       0.48      0.49      0.48       120
weighted avg       0.48      0.49      0.48       120


Let's look at differences between 'genres':
Accuracy on twitter data only: 0.5666666666666667
Accuracy on review data only: 0.4166666666666667
Confusion matrix of twitter data: 
[[11  5  4]
 [ 4 12  4]
 [ 1  8 11]]
Confusion matrix of review data: 
[[10  3  7]
 [11  1  8]
 [ 5  1 14]]
