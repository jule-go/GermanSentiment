Accuracy: 0.49166666666666664
F1-score: 0.48090300636383587
Confidence of model: 0.43[avg]	0.07[std dev]	0.343[min]	0.629[max]	0.41[median]
Confusion matrix: 
[[12 17 11]
 [ 6 26  8]
 [ 4 15 21]]
Built-in analysis: 
              precision    recall  f1-score   support

           0       0.55      0.30      0.39        40
           1       0.45      0.65      0.53        40
           2       0.53      0.53      0.53        40

    accuracy                           0.49       120
   macro avg       0.51      0.49      0.48       120
weighted avg       0.51      0.49      0.48       120


Let's look at differences between 'genres':
Accuracy on twitter data only: 0.5166666666666667
Accuracy on review data only: 0.4666666666666667
Confusion matrix of twitter data: 
[[ 4 11  5]
 [ 1 17  2]
 [ 1  9 10]]
Confusion matrix of review data: 
[[ 8  6  6]
 [ 5  9  6]
 [ 3  6 11]]
