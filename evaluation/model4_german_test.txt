Accuracy: 0.4083333333333333
F1-score: 0.40248480042294477
Confidence of model: 0.372[avg]	0.013[std dev]	0.34[min]	0.423[max]	0.373[median]
Confusion matrix: 
[[15  5 20]
 [13 12 15]
 [10  8 22]]
Built-in analysis: 
              precision    recall  f1-score   support

           0       0.39      0.38      0.38        40
           1       0.48      0.30      0.37        40
           2       0.39      0.55      0.45        40

    accuracy                           0.41       120
   macro avg       0.42      0.41      0.40       120
weighted avg       0.42      0.41      0.40       120


Let's look at differences between 'genres':
Accuracy on twitter data only: 0.4166666666666667
Accuracy on review data only: 0.4
Confusion matrix of twitter data: 
[[ 2  5 13]
 [ 3 12  5]
 [ 1  8 11]]
Confusion matrix of review data: 
[[13  0  7]
 [10  0 10]
 [ 9  0 11]]
